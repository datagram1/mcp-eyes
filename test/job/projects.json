[
  {
    "name": "Maps",
    "url": "http://maps.knws.co.uk/",
    "github": "https://github.com/datagram1/maps.git",
    "summary": "A complete self-hosted mapping and routing platform providing a fully private alternative to commercial mapping solutions. Features Docker-orchestrated microservices, OSRM routing engine, Nominatim geocoding, Leaflet-based map interface, and Flask API layer‚Äîall running entirely offline for secure internal networks.",
    "longDescription": "# üó∫Ô∏è **Local Web Map & OSRM Routing Platform**\n\n### *Long-Form Project Summary for KNWS Showcase*\n\nThis project is a complete self-hosted mapping and routing platform built using Docker-orchestrated components, OSRM routing, Leaflet-based map rendering, and a Flask-backed API layer. It provides a fully private, high-performance alternative to commercial mapping solutions such as Google Maps‚Äîideal for internal corporate networks, secure environments, or custom geospatial applications.\n\nThe system integrates fully offline routing, geocoding, and map display while being modular enough to support custom map tiles, additional data sources, or specialised navigation logic.\n\n---\n\n# üöÄ **Project Goals**\n\n1. **Deploy an entirely self-contained mapping service** without relying on Google, Mapbox, or external APIs.\n2. **Use Docker containers** to cleanly separate routing, geocoding, tile-serving, and web components.\n3. **Provide fast, offline routing** via OSRM (Open Source Routing Machine).\n4. **Expose a unified API layer** through Flask so that the mapping engine can be consumed by other internal systems.\n5. **Offer an intuitive, interactive map interface** for route planning, address lookup, and geospatial exploration.\n6. **Run reliably on local infrastructure**, inside a private LAN environment.\n\n---\n\n# üß† **Core Features**\n\n### **1. Interactive Map Interface (Leaflet)**\n\nThe frontend uses **Leaflet**, a lightweight and flexible JavaScript mapping library, to deliver:\n\n* Smooth map panning and zooming\n* Marker placement\n* Route overlays\n* Distance/route summaries\n* Popup info windows\n\nBecause the system is self-hosted, all tiles and data are served from your own infrastructure.\n\n---\n\n### **2. High-Performance Routing Engine (OSRM)**\n\nOSRM (Open Source Routing Machine) runs inside a dedicated Docker container and provides:\n\n* Lightning-fast turn-by-turn route generation\n* Car, foot, or cycling profiles\n* Polyline route output for map display\n* Distance and duration calculations\n* Support for locally hosted OpenStreetMap (OSM) extracts\n\nThe routing pipeline preprocesses the OSM file into `.osrm` datasets for near-instant routing performance.\n\n---\n\n### **3. Address Geocoding (Nominatim)**\n\nA stand-alone **Nominatim** container enables:\n\n* Forward geocoding (address ‚Üí coordinates)\n* Reverse geocoding (coordinates ‚Üí nearest address)\n* Full offline capability\n* Consistent, private geolocation services\n\nThis is ideal for internal corporate applications where cloud APIs are not permitted.\n\n---\n\n### **4. Flask API Layer**\n\nThe Flask application sits between the frontend and the containerised services, offering a unified interface:\n\n* `/route` ‚Üí queries OSRM for optimal routes\n* `/geocode` ‚Üí address lookup via Nominatim\n* `/reverse` ‚Üí coordinate-to-address conversion\n* `/tiles/...` ‚Üí tile serving if using custom tile containers\n* Authentication or logging hooks (optional)\n\nThis makes the system easily reusable as a backend for:\n\n* Fleet management\n* GIS tooling\n* Navigation devices\n* Custom logistic or delivery software\n\n---\n\n### **5. Docker-Based Microservice Architecture**\n\nEvery subsystem runs in its own Docker container:\n\n* **map_frontend** ‚Üí Leaflet + static files\n* **api_server** ‚Üí Flask application\n* **osrm_backend** ‚Üí OSRM routing engine\n* **nominatim** ‚Üí geocoding service\n* **tile_server** ‚Üí optional custom tile server\n\nThis ensures:\n\n* Isolation between modules\n* Easy upgrades\n* Reproducible builds\n* Clean deployment to new servers\n\nThe entire system can be rebuilt or relocated using a single `docker-compose.yml`.\n\n---\n\n### **6. Apache2 Frontend Integration**\n\nAn Apache2 reverse proxy exposes the application to users on your LAN:\n\n* SSL termination (optional)\n* URL routing (e.g., `/api`, `/tiles`, `/map`)\n* Load balancing or caching layers (if added)\n\nThis gives the map platform a stable, production-ready entry point.\n\n---\n\n# üõ† **Technical Architecture**\n\n### **Server Environment**\n\n* Ubuntu Server\n* Apache2 (reverse proxy / static hosting)\n* Docker + Docker Compose\n* LAN routing at: **192.168.10.10**\n\n### **Folder Structure**\n\n* `/frontend` ‚Üí Leaflet UI\n* `/backend` ‚Üí Flask API\n* `/routing` ‚Üí OSRM extraction and pre-processing scripts\n* `/data` ‚Üí OSM map extracts\n* `/docker` ‚Üí container definitions\n\n---\n\n# üîß **Pipeline & Workflow**\n\n1. **Download OSM map data** (e.g., regional extract).\n2. **Pre-process the extract using OSRM tools**:\n   `osrm-extract`, `osrm-partition`, `osrm-customize`.\n3. **Build containers** using Docker Compose.\n4. **Start the environment**:\n   OSRM, Nominatim, Flask, map frontend.\n5. **Serve via Apache2** for user access.\n6. **Use the browser** to search addresses, calculate routes, explore map tiles.\n\nEverything runs fully offline and can be hosted in air-gapped environments.\n\n---\n\n# üìà **Outcome & Impact**\n\nThis project demonstrates advanced capability in:\n\n* Geospatial systems\n* Docker microservice architecture\n* Routing and navigation algorithms\n* Backend API design\n* Frontend map visualisation (Leaflet)\n* Offline geocoding and mapping\n* Self-hosted infrastructure and DevOps\n* Apache2 integration & reverse proxy workflows\n\nThe final product is a robust, private alternative to commercial mapping platforms that can be deployed internally for logistics, planning, fleet management, surveying, or general geospatial development.",
    "technologies": {
      "frontend": [
        "html",
        "javascript",
        "leaflet"
      ],
      "backend": [
        "python",
        "flask"
      ],
      "database": [],
      "devops": [
        "docker",
        "apache",
        "osrm",
        "nominatim"
      ],
      "tools": [
        "openstreetmap"
      ]
    },
    "screenshot": "/screenshots/maps.png"
  },
  {
    "name": "Darts",
    "url": "https://darts.knws.co.uk",
    "github": "https://github.com/datagram1/darts.git",
    "summary": "A modern, responsive web-based dart game scorekeeper featuring an interactive SVG dartboard, professional announcer audio (Russ Bray), advanced scoring logic, checkout suggestions, and real-time statistics. Built as a lightweight browser-based SPA with Python-powered utilities for audio processing and future vision-based automated scoring.",
    "longDescription": "# üéØ **Dart Game Scorekeeper**\n\n### *Long-Form Project Summary for KNWS Showcase (Updated)*\n\n**Live version:** [https://darts.knws.co.uk](https://darts.knws.co.uk)\n\nThe Dart Game Scorekeeper is a modern, responsive, web-based scoring application for steel-tip darts featuring interactive gameplay, authentic announcer audio, advanced scoring logic, and a clean user experience. It is built as a lightweight browser-based SPA with no external dependencies, but is backed by Python utilities handling audio preprocessing and upcoming advanced features such as Vision Mode and real-time statistics.\n\nThis updated version integrates a **professional announcer pack**, improved UI, expanded scoring logic, and a roadmap of advanced modes powered by Python scripts and machine-vision tooling.\n\n---\n\n# üöÄ **Project Goals**\n\n1. Deliver a **fast, polished, zero-dependency darts scorekeeper** accessible from any device.\n2. Provide a **professional match experience** with real announcer audio (Russ Bray) and AI voice alternatives.\n3. Support advanced scoring features including **checkouts**, **stats**, **player history**, and **game flow automation**.\n4. Add **computer-vision automated scoring** (Vision Mode) as a future enhancement.\n5. Maintain a clean, responsive UX suitable for competitive or casual play.\n6. Serve the app as a **static single-page site** on Apache with optional SSL and Python utilities running server-side.\n\n---\n\n# üß± **Current Features (Live)**\n\n## üéØ **1. Manual Scoring Mode**\n\nThe live version includes full manual scoring support via:\n\n### **Interactive SVG Dartboard**\n\n* Click any dartboard segment to instantly register a score\n* Supports Singles, Doubles, Triples\n* Fully accurate hit-mapping using SVG coordinate regions\n* Tap-friendly mobile UX\n\n### **Manual Input Mode**\n\n* Number-pad score entry\n* Syncs perfectly with turn and leg logic\n* Suitable for rapid pub play or keyboard entry\n\n---\n\n## üîä **2. Professional Audio Announcer**\n\n### **Russ Bray Sound Pack (Pre-Recorded)**\n\nA full high-quality announcer sound pack is included, featuring:\n\n* Numbers 0‚Äì180\n* Calling for big scores (140+, 180)\n* Match-style emphasis and pacing\n\nPre-processing and timing alignment of clips were handled using Python utilities.\n\n### **AI Voice Announcers**\n\nFallback or alternative options:\n\n* Browser TTS\n* Kokoro AI voices (natural and expressive)\n\nPlayers can switch between announcers instantly.\n\n---\n\n## üßÆ **3. Advanced Scoring Logic**\n\n* Game modes: **301 / 501**\n* Straight In / Straight Out\n* Double In / Double Out\n* Bust logic\n* Turn history and undo/redo\n* Real-time scoreboard\n* Averages and per-turn scoring breakdown\n\n---\n\n## üéØ **4. Checkout / Finish Suggestions**\n\nAutomatically calculates finish routes for any score between **2 and 170**:\n\n* Multi-dart recommended finishes\n* Required doubles shown clearly\n* Improved UI for readable suggestions\n* Supports edge cases and complex outs\n\n---\n\n## üé® **5. Modern, Responsive UI**\n\n* Full dark mode\n* CSS Grid + Flexbox layout\n* Responsive from mobile to widescreen\n* Touch-friendly controls\n* Minimalistic, uncluttered design\n* Custom SVG graphics for clean board rendering\n\n---\n\n# üß™ **Python-Based System Extensions (Implemented but Not Yet Live)**\n\nAlthough the site runs as a static SPA, several **Python modules** power offline or future features. These include:\n\n### **1. Audio Processing Tools**\n\n* Batch trimming, normalising, and aligning Russ Bray announcer clips\n* Silence detection\n* Automatic phoneme grouping for smoother playback\n* Format conversion for browser-friendly playback\n\nThese were used to generate the professional voice pack.\n\n---\n\n### **2. Statistics Engine**\n\nPython scripts exist for:\n\n* Player averages\n* Checkout percentages\n* Three-dart averages\n* Momentum charts\n* Form consistency\n* Best leg / worst leg markers\n\nThese are functional backend modules awaiting front-end integration.\n\n---\n\n### **3. Vision Mode (WIP but functional in Python)**\n\nA future mode inspired by DartsMind:\n\n* Dartboard detection via OpenCV\n* Segment recognition using geometric transforms\n* Background subtraction\n* Dart tip detection / clustering\n* Debug overlays\n\nThe Python prototype already detects board segments and calculates score zones.\nThe frontend UI is prepared for future integration.\n\n---\n\n### **4. Match Playback / Replay Engine (WIP)**\n\nPython module for:\n\n* Turn-by-turn reconstruction\n* Animated overlay of past throws\n* Export to JSON for web playback\n\n---\n\n# üåê **Hosting & Infrastructure**\n\n* Hosted on **Apache 2.4**\n* Static SPA (HTML, JS, CSS)\n* SSL enabled via automated script (`setup-ssl.sh`)\n* Python utilities stored on server for preprocessing and future backend features\n* Lightweight, zero-maintenance deployment\n\n---\n\n# üõ† **Technology Stack**\n\n### **Frontend**\n\n* HTML5\n* CSS3 (Grid, Flexbox)\n* Vanilla JavaScript ES6\n* SVG graphics\n* Web Speech API\n* Audio sprite management for prerecorded announcer clips\n\n### **Backend / Utilities (Python)**\n\n* Python 3.x\n* OpenCV (for Vision Mode prototype)\n* NumPy\n* Pydub / FFmpeg (audio processing)\n* Custom scoring and stats modules\n\n### **Infrastructure**\n\n* Apache\n* SSL via Certbot\n* Static file optimisation\n\n---\n\n# üìà **Outcome & Impact**\n\nThe Dart Game Scorekeeper demonstrates:\n\n* Strong frontend engineering and SVG interactivity\n* Professional-grade audio integration\n* Hybrid architecture combining SPA simplicity with Python-powered advanced features\n* Deep understanding of darts scoring, checkout logic, and game rules\n* Vision-system prototyping skills (OpenCV)\n* High-quality UX for real-world use\n* Fully deployed, public project with ongoing feature expansion\n\nFuture updates will bring automated computer-vision scoring, player stats, and replay features powered by the Python modules already written.",
    "technologies": {
      "frontend": [
        "html5",
        "css3",
        "javascript",
        "svg"
      ],
      "backend": [
        "python"
      ],
      "devops": [
        "apache",
        "ssl"
      ],
      "tools": [
        "opencv",
        "numpy",
        "ffmpeg",
        "pydub"
      ]
    },
    "screenshot": "/screenshots/darts.png"
  },
  {
    "name": "News",
    "url": "https://news.knws.co.uk",
    "github": "https://github.com/datagram1/news.git",
    "summary": "A large-scale, fully automated news discovery and aggregation platform that collects articles daily from tens of thousands of global news sources. Features automatic categorization, historical archiving, intelligent deduplication, and a searchable Flask web interface for exploring global media output.",
    "longDescription": "# üì∞ **KNWS Global News Aggregation & Intelligence Platform**\n\n### *Long-Form Project Summary for KNWS Showcase*\n\nThis project is a large-scale, fully automated news discovery, aggregation, and classification system that collects articles daily from **tens of thousands of news sites worldwide**, processes them into structured categories, stores historical archives, and provides a searchable web interface for exploring global media output in one place.\n\nDesigned as an internal news intelligence engine, it blends high-volume RSS ingestion, web crawling, caching, deduplication, categorisation, async processing, and a Flask front-end for rich browsing and search.\n\nIt functions as a **private alternative to Google News + media intelligence platforms**, operating entirely on KNWS infrastructure with no third-party dependencies.\n\n---\n\n# üéØ **Project Goals**\n\n1. Build a **scalable news ingestion engine** capable of pulling headlines and stories from ~50,000 global news sources every day.\n2. Provide **automatic categorisation** into topics such as world news, politics, tech, business, sports, entertainment, local, etc.\n3. Maintain a **historical searchable archive** of all processed articles.\n4. Support **fluent keyword search**, date filtering, and source filtering.\n5. Deliver a streamlined **Flask-based news portal** for browsing, searching, and grouping stories.\n6. Use intelligent crawling logic for **avoid duplicates**, reduce noise, and handle broken feeds.\n7. Operate entirely self-hosted for privacy, reliability, and data longevity.\n8. Support future enhancements such as ML analytics, trend prediction, and mobile apps.\n\n---\n\n# üß† **Core Features**\n\n## üåç **1. Massive-Scale News Collection (50,000 sources)**\n\nThe system processes a huge dataset of global sources:\n\n* ~50k live news sites\n* Thousands of direct RSS feeds\n* Custom crawlers for sites without RSS\n* Real-time fetching\n* Duplicate detection\n* Fault tolerance for unreachable sites\n* Automatic feed discovery\n\nThe ingestion system runs continuously or on schedule, depending on configuration.\n\n---\n\n## ‚öôÔ∏è **2. Multi-Stage Ingestion Pipeline**\n\nThe ingestion architecture includes:\n\n### **RSS Sync Engine**\n\n* Handles large-scale RSS polling\n* Rate-limited to avoid bans\n* Normalises feed data into a unified structure\n* Stores metadata and story content\n\n### **Async Processing Layer**\n\n(Using asynchronous Python workers)\n\n* Fetching\n* Cleaning HTML content\n* Extracting titles, summaries, images\n* Extracting timestamps\n* Validating URLs\n* Categorising stories\n\n### **Crawler Framework**\n\nFor sites without RSS feeds:\n\n* Custom crawler classes per site\n* Pluggable architecture\n* HTML parsing with BeautifulSoup/lxml\n* Domain-specific rules\n* Automatic failover to generic scrapers\n\nDesigned for extensibility and easy addition of new sources.\n\n---\n\n## üß¨ **3. Automatic Categorisation**\n\nEvery article is sorted into one or more categories using:\n\n* Keyword heuristics\n* Site metadata patterns\n* Topic extraction\n* Title/summary analysis\n* Optional LLM-assisted tagging (offline-capable)\n\nCategories include:\n\n* World\n* Politics\n* Tech\n* Sports\n* Science\n* Business\n* Entertainment\n* Local news\n* Breaking stories (detection based on velocity)\n\n---\n\n## üï∞Ô∏è **4. Historical Archive**\n\nThe system maintains a long-term searchable record:\n\n* All articles stored to disk and database\n* Historical indexing\n* Search by:\n\n  * Keyword\n  * Date range\n  * Category\n  * Source\n  * Domain\n\nIdeal for research, media monitoring, or intelligence gathering.\n\n---\n\n## üîç **5. Search Interface (Flask Web Portal)**\n\nA fully-featured news browsing website, including:\n\n* Paginated article lists\n* Category filters\n* Search box with substring matching\n* Story detail view\n* Thumbnail display\n* Archives browser\n* Fast loading via cached assets\n\nTemplates are served statically for maximum performance.\n\n---\n\n## üíæ **6. Efficient Storage & Caching**\n\nThe system uses:\n\n* Disk-based caching for downloaded content\n* Log directories for ingestion analytics\n* Lightweight storage for raw HTML\n* Normalised text storage for indexing\n* Optimised filesystem layout for fast read/write\n* Optional SQLite/MySQL indexing\n\nLarge-scale news processing requires efficient I/O and caching strategies, all of which are present.\n\n---\n\n# üß∞ **Codebase Overview**\n\nThe repository includes a robust backend:\n\n### **Key Files**\n\n* `news.py` ‚Äî Main Flask web app\n* `config.py` ‚Äî Site and DB configuration\n* `sync_rss_feeds.py` ‚Äî Primary RSS sync engine\n* `sync_rss_feeds_class.py` ‚Äî Class-based modular sync system\n* `process_rss_async.py` ‚Äî Async ingestion worker\n* `utils.py` ‚Äî Shared functions for parsing, time handling, caching\n* `sites.csv` ‚Äî Database of **3,900+ RSS sources** (base list, extended via auto-discovery)\n* `crawlers/core` ‚Äî Base crawler architecture\n* `crawlers/modules` ‚Äî Site-specific crawlers\n* `downloaded/` ‚Äî Cached content\n* `logs/` ‚Äî Ingestion logs and error tracking\n* `templates/` and `static/` ‚Äî Full web UI\n\nThis modular design allows the system to scale horizontally and add new sources easily.\n\n---\n\n# üìà **Future / Planned Features**\n\n(Already sketched out in the README)\n\n* **Real-time notifications** (breaking news alerts)\n* **Social media sentiment integration**\n* **ML-based trend prediction**\n* **Multi-language support**\n* **API Gateway + rate limiting**\n* **Redis caching** for ultra-fast querying\n* **Message-queue-based ingestion** (e.g., RabbitMQ, Redis Streams)\n* **Docker containerisation**\n* **Kubernetes orchestration**\n* **Mobile app integration**\n* **Monitoring dashboards**\n* **User accounts + authentication**\n\nThese planned improvements position the platform for enterprise-scale media intelligence.\n\n---\n\n# üõ† **Technology Stack**\n\n### **Backend**\n\n* Python\n* Flask\n* Async I/O\n* Feedparser / Requests\n* BeautifulSoup / HTML parsing\n* CSV-driven data ingestion\n* Custom crawler classes\n* Disk caching\n* Optional DB integration\n\n### **Frontend**\n\n* HTML templates\n* CSS / JS\n* Responsive layout\n* Fast, minimal UI\n\n### **Infrastructure**\n\n* Self-hosted\n* Cron or systemd timers for ingestion\n* Log-based monitoring\n* Future Docker/Kubernetes support\n\n---\n\n# üìä **Outcome & Impact**\n\nThis platform demonstrates sophisticated engineering in:\n\n* Large-scale data ingestion\n* High-volume distributed crawling\n* Real-time classification\n* Text processing and content normalisation\n* Architecting scalable pipelines\n* Web UI development\n* Data storage and caching strategies\n* Designing for future microservices and distributed systems\n\nIt creates an internal news intelligence system far more powerful than typical RSS readers or news websites, and fully private within KNWS.",
    "technologies": {
      "backend": [
        "python",
        "flask"
      ],
      "database": [
        "sqlite",
        "mysql"
      ],
      "devops": [
        "docker",
        "kubernetes"
      ],
      "tools": [
        "beautifulsoup",
        "feedparser",
        "requests",
        "async"
      ]
    },
    "screenshot": "/screenshots/news.png"
  },
  {
    "name": "Speedtest",
    "url": "http://speedtest.knws.co.uk",
    "github": "https://github.com/datagram1/speedtest.git",
    "summary": "A modern, self-hosted Internet speed measurement platform with an elegant analog speedometer interface. Features comprehensive network diagnostics including ping, jitter, download/upload speed tests, IP detection, connection metadata, and a beautiful glassmorphism-themed UI with light/dark mode‚Äîall running entirely on KNWS infrastructure.",
    "longDescription": "# üöÄ **KNWS SpeedTest**\n\n### *Long-Form Project Summary for KNWS Showcase*\n\nA modern, self-hosted Internet speed measurement platform with a beautiful analog speedometer interface and detailed network diagnostics.\n\nKNWS SpeedTest was designed as an internal and public-facing performance tool to measure real-world connection quality across the KNWS infrastructure. It combines an elegant animated UI with accurate latency, jitter, download, and upload tests, plus IP and connection metadata detection ‚Äî all running on your own servers with no dependency on third-party APIs.\n\n---\n\n# üéØ **Project Goals**\n\n1. Provide a **self-hosted alternative** to commercial services like Ookla SpeedTest.\n2. Build a **classy, animated analog speedometer** inspired by classic car dashboards.\n3. Measure **ping, jitter, download speed, upload speed** with high accuracy.\n4. Detect and display **IP address, network provider, ASN, browser data, and connection type**.\n5. Provide KNWS with a reliable tool for **infrastructure testing, diagnostics, and customer support**.\n6. Ensure full compatibility across browsers and devices with a fast, minimal front-end.\n7. Present data in a **visually appealing, glassmorphism-themed UI** with light/dark mode.\n\n---\n\n# üß± **Core Features**\n\n## üß≠ **1. Analog Speedometer UI**\n\nA custom-crafted speedometer built with:\n\n* SVG graphics\n* Smooth CSS and JavaScript needle rotations\n* Tick marks and dynamic color zones\n* Full animation loop during tests\n* True analog-style inertia and motion easing\n\nThe design reflects automotive speedometers, providing a satisfying and intuitive experience.\n\n---\n\n## üåó **2. Dual Light/Dark Mode with Glassmorphism**\n\n* Instant theme switching\n* Theme stored in `localStorage`\n* Beautiful frosted-glass panels\n* Adaptive shadows and reflections per mode\n* Smooth fade transitions\n\nBoth modes have been hand-tuned for maximum readability and aesthetic impact.\n\n---\n\n## üì° **3. Full Network Quality Test Suite**\n\nThe application performs:\n\n### **Ping Test**\n\n* Multiple ICMP-like HTTP pings\n* Average latency\n* Jitter calculation (latency variance)\n\n### **Download Speed Test**\n\n* Fetches large random data blocks\n* Measures peak and sustained throughput\n* Calculates Mbps and records test duration\n\n### **Upload Speed Test**\n\n* Sends randomized payloads to server endpoints\n* Measures outbound throughput\n* Handles chunked uploads for consistency\n\nAll tests run in sequence and update the UI in real time.\n\n---\n\n## üåç **4. IP Address & Connection Metadata Detection**\n\nThe backend returns:\n\n* Public IP\n* ASN and ISP (if enabled via local lookup)\n* Approximate region (when configured)\n* Browser User-Agent\n* Downlink (via Network Information API)\n* Effective connection type (2G, 3G, 4G, 5G, Wi-Fi)\n* Estimated round-trip capability\n\nThis contextual info helps KNWS support diagnose user connectivity issues quickly.\n\n---\n\n## üìä **5. Detailed Results Dashboard**\n\nAt the end of each test, users see:\n\n* Ping (ms)\n* Jitter (ms)\n* Download speed (Mbps)\n* Upload speed (Mbps)\n* IP address and provider information\n* Timestamp\n* Browser version\n* Device type\n\nEverything is formatted in a clean card-based layout.\n\n---\n\n# üß™ **Architecture & Backend Logic**\n\nAlthough the frontend is a pure HTML/CSS/JS SPA, the backend provides endpoints for:\n\n* Download payloads\n* Upload receivers\n* Metadata queries\n* Time-sensitive ping simulation\n* Real-time diagnostic logs\n\nBackend tech stack typically includes:\n\n* Apache or Nginx\n* PHP or Python endpoints\n* Randomized data generators for speed accuracy\n* Reverse-proxy rules for consistent performance\n\nBecause the service is self-hosted, results reflect **true performance to the KNWS network**, rather than a generic CDN.\n\n---\n\n# üîß **Technology Stack**\n\n### **Frontend**\n\n* HTML5\n* CSS3 (Glassmorphism, Flexbox, animations)\n* JavaScript (ES6)\n* SVG analog gauge\n* LocalStorage\n* Network Information API\n\n### **Backend**\n\n* Apache (static hosting & upload endpoints)\n* PHP/Python for speed testing endpoints\n* Random data generators for throughput validation\n\n### **Infrastructure**\n\n* Runs entirely on KNWS internal servers\n* SSL-enabled\n* Zero third-party API calls\n\n---\n\n# üìà **Outcome & Value**\n\nThe KNWS SpeedTest platform provides:\n\n* Accurate, repeatable speed diagnostics\n* A beautiful, modern interface that users enjoy using\n* Fully private infrastructure (no data sent to external services)\n* A reliable tool for troubleshooting customer lines\n* A benchmark platform for server and network optimisation\n\nIt serves both internal engineering needs and public demonstration purposes, showcasing KNWS quality of service in a visually impressive way.",
    "technologies": {
      "frontend": [
        "html5",
        "css3",
        "javascript",
        "svg"
      ],
      "backend": [
        "php",
        "python",
        "apache"
      ],
      "database": [],
      "devops": [
        "apache",
        "ssl"
      ],
      "tools": []
    },
    "screenshot": "/screenshots/speed-test.png"
  },
  {
    "name": "Jobs/Contracts",
    "url": "http://jobs.knws.co.uk",
    "github": "https://github.com/datagram1/contracts.git",
    "summary": "AI-powered automated job application platform that intelligently finds, evaluates, and applies to contract jobs. Features multi-site scraping, AI-based scoring, automated applications, geolocation analysis, and professional cover letter generation.",
    "longDescription": "# Contracts - Automated Job Application Platform\n\n**Contracts** is a comprehensive, AI-powered platform that automates the entire lifecycle of finding, evaluating, and applying to contract jobs. It combines intelligent job scraping, AI-based scoring, automated applications, geolocation analysis, and professional cover letter generation into a unified system.\n\nThe platform saves significant time by automatically filtering thousands of job listings, identifying the most relevant opportunities, and handling application submissions‚Äîall while learning and optimizing search strategies through AI feedback.\n\n## üöÄ Core Functionality\n\n### Intelligent Job Discovery\n- Scrapes job postings from multiple sources (TotalJobs, Adzuna, Indeed, Reed, CV-Library, Jobsite)\n- Uses AI to generate optimized Boolean search queries\n- Browser extension for manual job tracking and coloring\n- Anti-bot handling with proxy rotation and TLS client\n\n### AI-Powered Evaluation\n- Processes your CV to extract skills and expertise\n- Scores jobs using hybrid AI (LLM primary + NLTK fallback)\n- Multi-stage filtering: job title fuzzy matching ‚Üí salary ‚Üí drive time ‚Üí skills ‚Üí AI scoring\n- Recommends best matches based on customizable thresholds\n\n### Geographic Intelligence\n- Converts job locations to coordinates via Nominatim\n- Calculates drive times using OSRM routing engine\n- Filters jobs by maximum acceptable commute time\n- Caches results for fast repeated lookups (20√ó faster with caching)\n\n### Automated Applications\n- Auto-applies to TotalJobs ApplyExpress positions\n- Handles authentication, form filling, and submission\n- Tracks application status in database\n- Extensible framework for additional job sites\n\n### Professional Document Generation\n- AI-powered cover letter generation using LLM\n- Professional PDF templates with custom branding\n- Smart GB-style greeting/sign-off logic\n- WeasyPrint rendering for high-quality PDFs\n\n### Centralized Dashboard\n- Flask web interface for managing jobs\n- User authentication (email/password, Google OAuth, Facebook OAuth)\n- Job organization: Applied, Not Applied, Guarded, Bin folders\n- Email notifications and verification\n\n### Learning & Optimization\n- Tracks which search queries find relevant jobs\n- Logs scoring performance (LLM vs NLTK)\n- Learns from user preferences and application history\n- Continuous improvement through feedback loops\n\n## üõ†Ô∏è Technology Stack\n\n### Backend\n- **Python 3.10+**: Core language\n- **Flask 3.1.1**: Web framework\n- **SQLAlchemy 2.0.42**: ORM for database operations\n- **MySQL 8.0**: Primary database\n\n### Web Scraping & Automation\n- **Playwright 1.54.0**: Browser automation\n- **Selenium 4.32.0**: Alternative browser automation\n- **requests 2.32.5+**: HTTP client\n- **aiohttp 3.12.14+**: Async HTTP client\n- **tls-client 1.0.1**: TLS client for anti-bot measures\n- **BeautifulSoup4 4.13.5**: HTML parsing\n\n### AI & Natural Language Processing\n- **vLLM**: Self-hosted LLM inference\n  - Models: Qwen2.5-Coder-32B-Instruct-AWQ, Llama-3.1-70B-Instruct\n- **NLTK 3.9.1**: Natural language processing\n- **spaCy 3.8.0+**: Advanced NLP\n- **rapidfuzz 3.14.3**: Fuzzy string matching\n- **scikit-learn 1.7.2**: Machine learning utilities\n\n### Document Processing\n- **pypdf 6.1.3**: PDF processing\n- **python-docx 1.2.0**: Word document processing\n- **WeasyPrint 66.0**: PDF generation\n- **Jinja2 3.1.6**: Template engine\n\n### Geolocation & Mapping\n- **Nominatim**: OpenStreetMap geocoding\n- **OSRM**: Open Source Routing Machine for drive time calculations\n- **geopy 2.4.1**: Geocoding library\n\n### Infrastructure\n- **APScheduler 3.10.4**: Background task scheduling\n- **Flask-Login 0.6.3**: User session management\n- **Flask-Mail 0.10.0**: Email functionality\n- **Flask-CORS 6.0.0**: Cross-origin resource sharing\n- **Tornado 6.5+**: Web server\n\n### Security\n- **Flask-Bcrypt 1.0.1**: Password hashing\n- **reCAPTCHA v3**: Bot protection\n- **pyOpenSSL 25.3.0**: SSL/TLS support\n\n## üìä Architecture\n\nThe platform consists of multiple layers:\n\n- **Job Ingestion Layer**: Multi-site scrapers, intelligent search, browser extension\n- **Job Enrichment Layer**: Geocoding, drive time calculation, proxy rotation\n- **Evaluation & Matching Layer**: CV processing, multi-stage filtering, hybrid AI scoring\n- **Application Layer**: Automated applications with browser automation\n- **Document Generation Layer**: AI-powered letter writing with professional PDF templates\n- **LLM Infrastructure**: Local and remote GPU deployments for AI inference\n\n## üéØ Key Benefits\n\n- **Time Savings**: Automatically filters thousands of irrelevant job ads\n- **Intelligence**: AI-powered matching based on skills and preferences\n- **Automation**: Hands-free application to suitable positions\n- **Optimization**: Learns and improves search strategies over time\n- **Context**: Geographic awareness with drive time calculations\n- **Professionalism**: Generates polished, branded cover letters\n- **Reliability**: Proxy rotation bypasses anti-scraping measures\n- **Centralization**: Single dashboard for entire job search process",
    "technologies": {
      "backend": [
        "python",
        "flask",
        "sqlalchemy"
      ],
      "database": [
        "mysql"
      ],
      "devops": [
        "docker",
        "tornado"
      ],
      "tools": [
        "playwright",
        "selenium",
        "vllm",
        "nltk",
        "spacy",
        "weasyprint",
        "nominatim",
        "osrm",
        "apscheduler"
      ]
    },
    "screenshot": "/screenshots/jobs-contracts.png"
  },
  {
    "name": "Web Tools",
    "url": "http://tools.knws.co.uk",
    "github": "https://github.com/datagram1/web_tools.git",
    "summary": "A comprehensive browser-based toolkit offering 93 utilities for converting, analyzing, compressing, and manipulating 300+ file formats. Features client-side WebAssembly processing for complete privacy, supporting audio/video/image/document conversion, OCR, compression, and developer utilities‚Äîall running entirely in the browser with offline capability.",
    "longDescription": "# üß∞ **Web Tools ‚Äì Comprehensive Web Utilities Suite**\n\n### *Long-Form Project Summary for KNWS Showcase*\n\nThe **Web Tools** platform is a large-scale, browser-based toolkit offering nearly one hundred utilities for converting, analysing, compressing, and manipulating over 300+ file formats. Designed as a modern, privacy-focused alternative to commercial sites like CloudConvert or Online-Convert, the entire suite runs **client-side**, powered by WebAssembly ‚Äî ensuring security, performance, and offline capability.\n\nBuilt with Flask for routing and organisation but executed entirely in the browser for compute-heavy tasks, the platform provides a full toolbox of converters, editors, analyzers, and everyday utilities, making it one of the most comprehensive free web tool collections available.\n\n---\n\n# üéØ **Project Goals**\n\n1. Build a **giant all-in-one conversion & utility platform** that replaces dozens of separate online tools.\n2. Support **hundreds of file formats** across images, audio, video, documents, archives, and more.\n3. Ensure **100% privacy** with **no server-side file uploads** (all processing runs in-browser via WASM).\n4. Provide a **fast, responsive user experience** on any device.\n5. Offer **a curated set of general-purpose utilities** (calculators, regex tools, text cleaners, encoders, etc.).\n6. Make the entire suite **free**, with no signups or limits.\n7. Architect the system so users can run many tools **offline** after initial load.\n8. Compete with high-end commercial platforms‚Äîwithout storing user data.\n\n---\n\n# üß± **Core Highlights**\n\n## üß© **1. 93 Tools Across 12 Categories**\n\nTools include:\n\n* Audio conversion (MP3, WAV, FLAC, AAC, OGG‚Ä¶)\n* Image conversion (PNG, JPG, WebP, GIF, SVG‚Ä¶)\n* Video conversion (MP4, MKV, MOV, AVI‚Ä¶)\n* Document conversion (PDF, DOCX, TXT, Markdown‚Ä¶)\n* Archive utilities (ZIP, TAR, 7z, RAR extract)\n* OCR (image & PDF text extraction) via Tesseract WASM\n* Image compression & resizing\n* Audio trimming, merging, and normalisation\n* Video compression & frame extraction\n* Base64 tools\n* Hashing utilities\n* Random generators\n* Text manipulation (case conversion, deduplication, cleanup)\n* Developer tools (JSON formatter, regex tester, YAML ‚Üî JSON, CSV tools)\n\nThe conversion matrix covers **300+ file formats**, with near-instant in-browser processing.\n\n---\n\n## ‚öôÔ∏è **2. WebAssembly Processing (Privacy-First Architecture)**\n\nThe suite uses WebAssembly modules to run heavy processing directly in the user's browser. This includes:\n\n* **FFmpeg WASM** for video/audio processing\n* **Tesseract WASM** for OCR\n* **Squoosh codecs** for advanced image compression\n* **WebAssembly file parsing libraries**\n\nThis architecture ensures:\n\n* No file ever leaves the user's device\n* Superior performance vs server-based conversion\n* Offline capability\n* No load on the server for conversions\n\nThe Flask backend serves only HTML, JS, and WASM modules ‚Äî everything else is client-side.\n\n---\n\n## üõ†Ô∏è **3. Utility Tools Beyond File Conversion**\n\nIn addition to format conversions, the suite includes general-purpose utilities such as:\n\n* Colour pickers\n* Image metadata analysers\n* Audio waveform visualisers\n* Hash calculators (MD5, SHA1, SHA256, etc.)\n* Timestamp converters\n* Base64, URL, and HTML encoders/decoders\n* Password / key generators\n* Unicode and emoji tools\n* Network/IP utilities\n* String diff comparison tools\n\nThese tools make Web Tools a broad productivity environment, not just a file converter.\n\n---\n\n## üåê **4. Offline-Ready Operation**\n\nOnce loaded, many tools work fully offline because:\n\n* The WASM modules run locally\n* File operations do not require a server\n* JS logic is fully self-contained\n\nThis makes the suite ideal for slow or unreliable connections ‚Äî or complete offline environments.\n\n---\n\n## üåì **5. Modern, Responsive Front-End**\n\nBuilt with a focus on UX:\n\n* Dark mode with system preference detection\n* Smooth animations\n* Mobile-first responsive design\n* Fast navigation between tools\n* Zero-refresh transitions\n* Clean layout with intuitive category structure\n\nUsers can find tools quickly with search and category filters.\n\n---\n\n## üîß **6. Flask Backend (Routing & Organisation)**\n\nWhile most logic runs in the browser, Flask provides:\n\n* Routing\n* Tool indexing\n* Metadata definition\n* Category structure\n* Static WASM file serving\n* \"Web suite\" organisation for dozens of tools\n\nMinimal server overhead, maximum front-end performance.\n\n---\n\n## üìÇ **Architecture Snapshot**\n\n```\nFlask (routes, static hosting)\n   ‚Üì\nHTML / JS / CSS\n   ‚Üì\nWebAssembly modules (FFmpeg, Tesseract, Squoosh, custom WASM)\n   ‚Üì\nBrowser-based computation\n   ‚Üì\nNo server-side file handling ‚Äî total privacy\n```\n\n---\n\n# üõ†Ô∏è **Technology Stack**\n\n### **Frontend**\n\n* HTML5\n* CSS3 / responsive layout\n* JavaScript ES6 modules\n* WebAssembly (FFmpeg, Tesseract, Squoosh codecs, WASM utilities)\n* LocalStorage for dark-mode preferences\n\n### **Backend**\n\n* Flask (Python)\n* Simple filesystem hosting for WASM modules\n* Optional logging for usage analytics (no file uploads logged)\n\n### **Infrastructure**\n\n* Apache / Nginx compatible\n* Fully static-serving capable\n* Extremely low server cost\n\n---\n\n# üìà **Outcome & Impact**\n\nThe Web Tools suite provides:\n\n* A powerful, privacy-focused alternative to cloud conversion platforms\n* A **free, offline-capable** toolkit with dozens of utilities\n* A WebAssembly-powered processing engine delivering high performance\n* A unified platform that replaces dozens of separate single-purpose websites\n* Zero server compute cost for conversions\n* A valuable tool set for developers, students, engineers, and general users\n\nThis project demonstrates expertise across:\n\n* WebAssembly integration\n* Front-end engineering at scale\n* Multi-format file processing\n* UX design for large tool suites\n* Flask backend structuring\n* Privacy-by-design architecture",
    "technologies": {
      "frontend": [
        "html5",
        "css3",
        "javascript",
        "webassembly"
      ],
      "backend": [
        "python",
        "flask"
      ],
      "database": [],
      "devops": [
        "apache",
        "nginx"
      ],
      "tools": [
        "ffmpeg",
        "tesseract",
        "squoosh"
      ]
    },
    "screenshot": "/screenshots/web-tools.png"
  },
  {
    "name": "GitHub Trending",
    "url": "http://github.knws.co.uk",
    "github": "https://github.com/datagram1/github_trending.git",
    "summary": "A high-performance, self-hosted GitHub trending analytics platform that collects, processes, and visualizes activity across the global GitHub ecosystem. Features daily updates, interactive leaderboards, trend analysis, and a modern Next.js dashboard running on KNWS infrastructure.",
    "longDescription": "# ‚≠ê **GitHub Trending & Developer Ecosystem Analytics Platform**\n\n### *Long-Form Project Summary for KNWS Showcase*\n\nA high-performance, self-hosted GitHub trending analytics engine that collects, processes, stores, and visualises activity across the global GitHub ecosystem ‚Äî updated daily and served via a modern Next.js dashboard running on the KNWS internal web infrastructure.\n\nThis platform reveals the **top repositories, fastest-growing projects, trending topics, languages, and activity patterns** across GitHub, with a fully custom backend ingestion pipeline, long-term snapshots database, and beautifully designed interactive UI.\n\n---\n\n# üéØ **Project Goals**\n\n1. Provide a **self-hosted alternative** to GitHub Trending with far richer data.\n2. Track **all GitHub activity** (stars, forks, new repos, contributors, topics) daily.\n3. Build a **fully automated ingestion + analytics pipeline** for long-term trend tracking.\n4. Present interactive **leaderboards, charts, stats, and growth visualisations**.\n5. Serve the frontend from a **local KNWS webserver** with no third-party dependencies.\n6. Offer a clean, modern, responsive UI built on the latest web technologies.\n7. Maintain historical data for months/years to enable trend analysis and discovery.\n8. Provide insights into rising ecosystems, languages, frameworks, and developer trends.\n\n---\n\n# üîß **Backend: High-Speed Trending Ingestion Engine (Quart + Python)**\n\nThe backend is an advanced Python 3.14+ application built on **Quart** (an async Flask-compatible framework), designed for high-throughput scraping, API ingestion, and database insertion.\n\n### Key backend capabilities:\n\n### **1. Distributed Ingestion Engine**\n\n* Fully asynchronous collection of trending data\n* Fetching repository metadata, stars, forks, topics\n* Multi-worker pipeline for throughput\n* Rate-limit aware request scheduling\n* Exponential backoff and retry system\n* Thousands of repos processed per cycle\n\n### **2. Daily Metrics Snapshotting**\n\nEvery day, the system stores:\n\n* Stars gained\n* New forks\n* Contributor delta\n* Open issue counts\n* Language metadata\n* Topics\n* Rank movements\n\nThis produces long-term datasets ideal for leaderboards and visualisation.\n\n### **3. Queue + Fallback System**\n\nA custom internal event-queue ensures:\n\n* Reliable processing\n* Automatic failover\n* No data loss during GitHub API rate limits\n* Graceful skipping & resume logic\n\n### **4. Topic & Language Discovery**\n\nThe system auto-detects:\n\n* Trending programming languages\n* Rising topics\n* New ecosystems\n* Technology clusters (e.g., AI, WASM, automation tools)\n\nIt uses metadata extraction + fallback scraping when APIs are missing.\n\n### **5. PostgreSQL 15+ Database**\n\nBackend storage includes:\n\n* `repositories` ‚Äî core metadata\n* `snapshots` ‚Äî daily metrics\n* `trending` ‚Äî leaderboard entries\n* `languages` ‚Äî language statistics\n* `topics` ‚Äî topic clusters\n* `history` ‚Äî long-term archival tables\n\nIndexed for fast query performance under heavy dashboard load.\n\n---\n\n# üé® **Frontend: Modern Developer Dashboard (Next.js + React)**\n\nA completely modern dashboard built with:\n\n### **Frameworks & Libraries**\n\n* **Next.js 15.5** (App Router, server components, Turbopack)\n* **React 19.1**\n* **TypeScript 5**\n* **Tailwind CSS 4**\n* **shadcn/ui** components\n* **Lucide & Radix icons**\n\nThe UI is extremely smooth, fast, and mobile-first.\n\n### **Dashboard Features**\n\n* Interactive trending leaderboards\n* Filters:\n\n  * Time range (24h, weekly, monthly)\n  * Language\n  * Topic\n  * Stars/forks gained\n* Repository detail cards\n* Historical charts (sparkline graphs)\n* Daily movement indicators\n* Dark/light mode with system auto-detect\n\n### **Visualisation Tools**\n\n* Server-rendered charts for performance\n* Real-time stat updates from the backend\n* Animations using CSS + React transitions\n* Sorting by multiple metrics:\n\n  * Star velocity\n  * Fork velocity\n  * Contributors growth\n  * Ranking delta\n\nEverything is hosted on the **KNWS local webserver**, ensuring privacy, speed, and reliability.\n\n---\n\n# üß© **Architecture Overview**\n\n```\nGitHub ‚Üí Async Ingestion Engine (Quart) ‚Üí PostgreSQL Snapshots ‚Üí\n   Next.js Dashboard ‚Üí User Browser\n```\n\n### Pipelines:\n\n* Cron/worker triggers ingestion\n* Python engine fetches trending repos\n* Data enters queue ‚Üí processed ‚Üí stored\n* Next.js fetches daily structured API endpoints\n* Dashboard renders leaderboards + charts\n\n---\n\n# üìä **Data Insights Provided**\n\nUsers can explore:\n\n* Fastest growing repos today\n* Top weekly and monthly projects\n* Trending languages\n* Rising topics/technologies (AI, DevOps, JS frameworks, etc.)\n* New repos exploding in popularity\n* Historical performance for any repository\n* Early-stage projects gaining momentum\n* Domain-specific fields (e.g., LLMs, OS tooling, game engines)\n\nIdeal for developers tracking ecosystems, companies scouting talent, or hobbyists exploring GitHub trends.\n\n---\n\n# üõ†Ô∏è **Tech Stack Summary**\n\n### **Backend**\n\n* Python 3.14+\n* Quart (async Flask-compatible framework)\n* Psycopg 3\n* Playwright (optional scraping)\n* PostgreSQL 15+\n* Async workers + queues\n\n### **Frontend**\n\n* Next.js 15.5\n* React 19.1\n* TypeScript 5\n* Tailwind CSS 4\n* shadcn/ui\n* Lucide + Radix Icons\n* Turbopack dev tooling\n\n### **Infrastructure**\n\n* KNWS local webserver\n* Docker optional\n* TLS/SSL via Apache/Nginx\n* Cron/systemd timers for ingestion\n\n---\n\n# üìà **Outcome & Impact**\n\nThe KNWS GitHub Trending platform provides:\n\n* Real-time insight into the fastest-growing projects on GitHub\n* A beautiful, modern dashboard experience\n* Long-term historical data not provided by GitHub itself\n* Internal developer intelligence for KNWS\n* A fully private, self-hosted analytics platform\n* A powerful tool for discovering rising stars in development ecosystems\n\nIt showcases your ability to combine distributed data ingestion, async Python, advanced database design, and cutting-edge frontend frameworks into one cohesive product.",
    "technologies": {
      "frontend": [
        "next.js",
        "react",
        "typescript",
        "tailwindcss"
      ],
      "backend": [
        "python",
        "quart",
        "postgresql"
      ],
      "database": [
        "postgresql"
      ],
      "devops": [
        "docker"
      ],
      "tools": [
        "playwright",
        "psycopg"
      ]
    },
    "screenshot": "/screenshots/github-trending.png"
  },
  {
    "name": "vLLM",
    "url": null,
    "github": "https://github.com/datagram1/vllm.git",
    "summary": "A high-throughput, memory-efficient LLM inference engine integrated into KNWS infrastructure. Features optimized attention mechanisms (PagedAttention), continuous batching, quantization support, distributed inference, and a vLLM manager web interface for monitoring and control. Enables real-time semantic analysis at scale with low latency.",
    "longDescription": "# Updated Project Summary ‚Äì Integrating vLLM Tech Stack\n\nThis project extended and integrated the vLLM inference/serving engine to build a custom LLM-powered module in your system. You adapted and deployed the vLLM stack to support your in-house AI capabilities (e.g., for skills/scoring in the Job Aggregator, or other semantic text-analysis tasks). Key tech stack details from the vLLM project include:\n\n---\n\n## Key Capabilities of vLLM\n\n* vLLM is a high-throughput, memory-efficient library designed for large language model serving and inference.\n* It supports optimized attention mechanisms (PagedAttention), continuous batching, quantization (GPQT, AWQ, INT4, INT8, FP8), distributed inference (tensor/pipeline/expert parallelism) and a wide variety of hardware (GPUs, TPUs, CPUs) and frameworks.\n* The repository lists major languages and components: Python (~85.9%), Cuda (~7.9%), C++ (~4.5%) among others.\n* The broader \"Production Stack\" repository supports Kubernetes (Helm charts), request routing, observability (Prometheus/Grafana), autoscaling, KV-cache sharing, caching layers, distributed inference, etc.\n\n---\n\n## Your Contribution & Integration\n\n* You **adapted vLLM** within your system as the AI semantic analysis engine that underpins features like job-skills matching, search profile tuning, and application-classification.\n* You configured a **local vLLM instance** (or cluster) via the vLLM production stack architecture (or inspired by it) to run in your private infrastructure ‚Äî integrating request routing, caching, and model serving.\n* You built custom adapters to weigh job-skills, semantic categories, user profiles and applied the LLM output into your scoring engine.\n* You extended the system to integrate with your search and ingestion pipelines, enabling fast inference for every job listing in near-real time.\n* You implemented monitoring and metrics for your LLM stack (latency, throughput, token counts), drawing on the reference observability tools described in the vLLM repo.\n\n---\n\n## Tech Stack Summary (incorporating vLLM components)\n\n**Languages / Frameworks:**\n\n* Python (core logic, LLM adapters, orchestration)\n* C++ / CUDA (as part of the vLLM engine, GPU kernels)\n* Bash / shell scripting (deployment, orchestration)\n* YAML / Helm (Kubernetes deployment, if used)\n\n**Libraries / Systems:**\n\n* vLLM library (LLM inference engine)\n* PyTorch (underlying model framework)\n* Hugging Face model loading support (via vLLM)\n* Quantization libraries / kernels (INT4, INT8, FP8)\n* OpenAI-compatible API layer (in vLLM)\n* Monitoring: Prometheus + Grafana (observability stack)\n* Containerisation: Docker / Kubernetes (Helm charts)\n* Routing layer: request router / multi-model routing (vLLM production stack)\n\n**Hardware / Infrastructure:**\n\n* GPU inference (NVIDIA CUDA / AMD / other hardware)\n* Distributed inference / multi-replica serving\n* KV-cache reuse / memory-efficient attention (PagedAttention)\n* Cluster orchestration for scaling\n\n**Deployment / DevOps:**\n\n* Helm charts for deployment on K8s\n* CI/CD pipelines (build, test, deploy)\n* Metrics dashboards (latency, throughput, TKFT)\n\n---\n\n## Outcome & Significance\n\nBy integrating vLLM into your system, you elevated your platform's capability in several ways:\n\n* Enabled **real-time semantic analysis** and classification of data at scale (jobs, news, etc).\n* Achieved **high throughput inference** with low latency, leveraging vLLM's memory-efficient architecture.\n* Built a **private inference stack** within your company infrastructure ‚Äî avoiding cloud vendor lock-in and reducing data exposure.\n* Positioned your system to **scale** (via containerised deployment, routing, caching) as model size or traffic grows.\n* Demonstrated expertise in advanced LLM serving, distributed systems, container orchestration, and ML infrastructure.",
    "technologies": {
      "frontend": [
        "html"
      ],
      "backend": [
        "python",
        "c++",
        "cuda"
      ],
      "database": [],
      "devops": [
        "docker",
        "kubernetes",
        "helm",
        "prometheus",
        "grafana"
      ],
      "tools": [
        "vllm",
        "pytorch",
        "huggingface"
      ]
    },
    "screenshot": "/screenshots/vllm.png"
  },
  {
    "name": "MCP Eyes",
    "url": null,
    "github": "https://github.com/datagram1/mcp-eyes.git",
    "summary": "A fully-featured Model Context Protocol (MCP) automation server that acts as a universal control layer between LLMs and real-world computer automation tasks. Enables AI agents to perform complex automation workflows on macOS and Linux through screen capture, UI inspection, window management, file operations, and command execution.",
    "longDescription": "# ü§ñ **MCP Automation Server for Local & Cloud LLMs**\n\n### *Long-Form Project Summary for KNWS Showcase*\n\nThis project is a fully-featured **Model Context Protocol (MCP) automation server**, built to act as a universal control layer between **LLMs** (local or cloud-hosted) and **real-world computer automation tasks**.\n\nIt empowers an LLM to perform complex automation workflows on macOS and Linux, integrating screen capture, UI inspection, window management, file operations, command execution, and future ML-based element detection. The goal is to provide an AI agent with structured, safe, precise access to the user's environment via the standard MCP tool interface.\n\nBuilt as a modular, extensible server, it runs locally or on remote machines, allowing LLMs to control applications, inspect UI states, and execute tasks with fine-grained permissioning and strong sandboxing.\n\n---\n\n# üéØ Project Goals\n\n1. Build a universal **MCP-compatible automation server** that exposes system-level tools to any LLM.\n2. Provide **cross-platform (macOS/Linux)** functionality for UI automation, screen capture, and OS integration.\n3. Allow LLMs to perform **real-world actions** such as clicking, typing, launching apps, manipulating windows, or querying system state.\n4. Implement a **tool-based, permission-aware interface** that keeps the user in control and maintains safety.\n5. Lay the foundation for **advanced AI agent capabilities**, including ML-driven visual detection and workflow recording.\n6. Support integration with LLMs running **locally (vLLM, LM Studio, Ollama)** or **in the cloud (OpenAI, Anthropic, etc.)**.\n\n---\n\n# üß± Core Features\n\n## üñ•Ô∏è **1. System-Level Automation Tools**\n\nExpose system-level controls to LLMs through MCP tools:\n\n* Programmatic mouse movement\n* Clicking / double-click / drag\n* Keyboard entry\n* App launching\n* Window switching\n* Window geometry manipulation\n* Clipboard access\n* File I/O utilities\n\nTools are structured with clear schemas so LLMs know precisely how to call them.\n\n---\n\n## üñºÔ∏è **2. Screenshot & Screen Inspection**\n\nFull screenshot capabilities:\n\n* Entire display capture\n* Per-display screenshot\n* Window-specific capture (macOS)\n* Multi-monitor support\n* Configurable resolution for performance\n\nScreenshots are returned in formats suitable for:\n\n* Vision-enabled LLMs\n* Automation scripts\n* ML-based UI element detection\n\nSpecial handling for macOS permissions (TCC), with troubleshooting guides included.\n\n---\n\n## ü™ü **3. Window & Process Management**\n\nLLMs can interact with:\n\n* Running processes\n* Application state\n* Window titles\n* Window geometry\n* Focus and display placement\n\nThis creates a bridge for advanced automation such as:\n\n* \"Find the Chrome window and bring it to the front\"\n* \"Resize this app to the left side of the screen\"\n* \"Close all Finder windows\"\n\n---\n\n## ‚öôÔ∏è **4. Command Execution Sandbox**\n\nA safe, sandboxed execution layer:\n\n* Shell commands\n* Python snippets\n* System utilities\n* Configuration commands\n\nThis makes it possible for an LLM to solve problems autonomously while following user-defined safety rules.\n\n---\n\n## üîå **5. Hardware and OS Integration**\n\nThe server integrates tightly with host OS capabilities:\n\n* macOS Accessibility APIs\n* Linux X11/Wayland utilities\n* System permissions frameworks\n* TCC (macOS Transparency, Consent & Control)\n\nThe README includes fixes for:\n\n* ScreenRecording permission denial\n* \"Operation not permitted\" errors\n* Windowing API access issues\n\n---\n\n# üîÆ Future Features (From README Roadmap)\n\nPlanned but not yet implemented, many with partial Python prototypes:\n\n* **Plugin system** for custom tools\n* **Web-based configuration interface**\n* **High-performance screenshot engine**\n* **Advanced multi-display window management**\n* **UI state monitoring (polling/event-based)**\n* **Conditional workflow automation (IF/THEN logic)**\n* **ML-based visual element detection**\n* **Browser extension for web-specific automation**\n* **Record/playback of automation workflows**\n* **Distributed multi-machine automation**\n\nThese upgrades position the server as a full AI automation framework.\n\n---\n\n# üß∞ Technology Stack\n\n### **Languages**\n\n* Python (primary)\n* Bash/shell scripting\n* Swift bridging for macOS where required (planned)\n\n### **Frameworks / APIs**\n\n* MCP (Model Context Protocol)\n* macOS TCC / Accessibility APIs\n* Linux automation utilities\n* PyObjC (optional for macOS)\n* pillow / image processing\n* subprocess for command execution\n\n### **Infrastructure**\n\n* Local and remote LLMs (OpenAI, Anthropic, vLLM, LM Studio, Ollama)\n* System daemon / background service capabilities\n* Configuration via YAML/JSON\n\n### **Automation & Control**\n\n* Input automation (mouse/keyboard)\n* Window management APIs\n* Screenshot pipeline\n* Safe shell command execution\n\n---\n\n# üìà Outcome & Impact\n\nThe MCP automation server provides:\n\n* A **unified tool layer** letting any LLM become a functional OS automation agent\n* Cross-platform support for both macOS and Linux\n* A sandboxed, permission-based environment for secure automation\n* The foundation for advanced AI-driven workflows\n* Improved developer productivity through hands-free control of apps and systems\n* Compatibility with local GPU LLMs such as vLLM or Ollama\n\nIt represents a major step toward **fully autonomous AI desktop agents**, securely bridging LLMs with real operating system capabilities.",
    "technologies": {
      "backend": [
        "python"
      ],
      "devops": [],
      "tools": [
        "mcp",
        "npm",
        "pyobjc",
        "pillow"
      ]
    },
    "screenshot": "/screenshots/mcp-eyes.png"
  },
  {
    "name": "Proxy Router",
    "url": null,
    "github": "https://github.com/datagram1/proxyrouter.git",
    "summary": "A high-performance proxy aggregation and routing engine written in Go, designed to manage, validate, rotate, and route traffic through tens of thousands of proxies simultaneously. Features automatic proxy rotation, intelligent routing logic, high-throughput concurrency, REST API, Firefox extension, and multi-platform Docker support.",
    "longDescription": "# üåê **High-Throughput Proxy Router & Multi-Proxy Outbound Network Engine (Go-Based)**\n\n### *Long-Form Project Summary for KNWS Showcase*\n\nThis project is a **high-performance proxy aggregation and routing engine** written primarily in **Go**, designed to download, manage, validate, rotate, and route traffic through **tens of thousands of proxies** simultaneously. It acts as a **network router** for multiple machines or applications, enabling them to send traffic through large rotating proxy pools for privacy, security, research, scraping, or load distribution.\n\nThe system is engineered for **extreme concurrency**, supporting thousands of simultaneous outbound connections, dynamically rotating proxies, and intelligent routing rules for speed, resiliency, and anonymity.\n\n---\n\n# üéØ **Project Goals**\n\n1. Build a **massively parallel proxy ingestion and routing system**, capable of handling tens of thousands of proxies of different types (HTTP, HTTPS, SOCKS4/5).\n2. Allow multiple client machines or apps to route outbound traffic through a **single unified router**.\n3. Provide **automatic proxy rotation** for every outbound request, ensuring anonymisation and distributing load.\n4. Implement **custom routing logic**, such as IP-based routing overrides, sticky sessions, and fallback behaviour.\n5. Support **high-throughput, low-latency** connection forwarding suitable for large scraping systems or security use cases.\n6. Include additional tooling such as a Firefox extension, Python clients, and metrics APIs.\n7. Ship with **multi-platform builds**, Docker support, environment automation, and cross-platform deployability.\n\n---\n\n# üß± **Core Features**\n\n## üåç **1. Proxy Aggregation: Tens of Thousands of Sources**\n\nThe engine can:\n\n* Download proxy lists from large numbers of public or private sources\n* Parse mixed formats (IP:Port, JSON, CSV, provider-specific formats)\n* Validate proxy health and speed\n* Maintain a massive internal proxy pool\n* Categorise proxies by protocol, speed, reliability, region, anonymity\n\nThe resulting proxy list is continuously refreshed and quality-ranked.\n\n---\n\n## üîÄ **2. Automatic Proxy Rotation**\n\nEvery outbound connection routed through the server will:\n\n* Use a unique proxy\n* Rotate on every request\n* Recycle only healthy proxies\n* Avoid banned/burned IPs\n* Optionally maintain sticky sessions where needed\n\nThe rotation logic ensures even distribution and avoids reuse patterns that can trigger rate limits or detection systems.\n\n---\n\n## üåê **3. Acts as a Full Network Router for Multiple Clients**\n\nYour system serves as a **network-level proxy router**, where:\n\n* Multiple devices\n* Multiple apps\n* Entire scraping clusters\n\ncan point traffic at a single endpoint and have it transparently routed through rotating proxies.\n\nExamples:\n\n* A local browser's requests\n* Backend servers\n* Cron jobs\n* LLMs or automation agents fetching data\n\nThis effectively turns your router into a **traffic anonymisation gateway**.\n\n---\n\n## üß© **4. High-Performance Go Architecture**\n\nThe engine uses:\n\n* **Goroutines** for extreme concurrency\n* **Channels** for safe, scalable async data flows\n* **go-chi/chi** for API routing\n* **modernc.org/sqlite** (pure-Go sqlite)\n* **armon/go-socks5** for SOCKS5 server functionality\n* **Custom HTTP CONNECT tunnelling**\n* Memory-safe, low-overhead patterns\n\nThe architecture is optimised for handling **thousands of simultaneous outbound requests**.\n\n---\n\n## üì¶ **5. Multiple Proxy Types Supported**\n\nThe router supports:\n\n* HTTP\n* HTTPS\n* SOCKS4\n* SOCKS5\n* Custom dialects (provider-specific)\n\nThis enables compatibility across the entire proxy ecosystem.\n\n---\n\n## üß† **6. Intelligent Routing Logic**\n\nThe system includes logic for:\n\n* Auto fallback when a proxy fails\n* Timeout statistics\n* Geolocation-aware routing\n* IP-based rulesets\n* Whitelisting/blacklisting\n* Special-case routing for specific domains\n* Sticky sessions (optional)\n* Per-client proxy isolation\n\nDocumentation exists in:\n\n* `SPECIAL_IP_ROUTING_README.md`\n* `CUSTOM_PROXY_IMPLEMENTATION_SUMMARY.md`\n\n---\n\n## üìä **7. Monitoring & Timeout Statistics API**\n\nA built-in stats API provides:\n\n* Success/failure rates\n* Average response times\n* Proxy health monitoring\n* Timeout counts\n* Connection throughput metrics\n\nA Python client is bundled for integration with external systems.\n\n---\n\n## üîå **8. Firefox Extension (Optional Component)**\n\nThe repository includes a full Firefox extension:\n\n* Routes browser traffic through the proxy router\n* Toggles routing on/off\n* Displays proxy rotation indicators\n* Can apply custom rules per site\n\nIdeal for testing, browsing through rotating proxies, or UI-based validation.\n\n---\n\n## üê≥ **9. Dockerised Multi-Platform Build System**\n\nThe project includes:\n\n* Multi-architecture builds (amd64, arm64)\n* Docker Compose files\n* Automated `.env` loader\n* Environment propagation into containers\n* Cross-platform build scripts in `/builds/`\n\nMaking deployment simple on:\n\n* Proxmox\n* Linux servers\n* macOS development machines\n* Raspberry Pi clusters\n\n---\n\n# üõ†Ô∏è **Technology Stack**\n\n### **Primary Language**\n\n* **Go (Golang)** ‚Äì concurrency-heavy proxy routing logic\n\n### **Libraries**\n\n* `go-chi/chi` ‚Äì REST API/HTTP routing\n* `modernc.org/sqlite` ‚Äì pure-Go SQLite (no cgo required)\n* `armon/go-socks5` ‚Äì SOCKS5 server implementation\n* Custom Go modules for:\n\n  * Proxy validation\n  * Rotation\n  * Connection multiplexing\n  * Timeout logging\n\n### **Supporting Components**\n\n* Python client tools\n* Bash utilities\n* Firefox WebExtension\n* Docker + multi-platform builds\n* MySQL (infrastructure layers for user mgmt / logs)\n\n### **Infrastructure**\n\n* Runs as a local or remote router\n* Supports thousands of concurrent outbound sessions\n* Instant proxy rotation and failover\n* Works with any client supporting HTTP/SOCKS5 proxies\n\n---\n\n# üìà **Outcome & Impact**\n\nThis project demonstrates deep knowledge in:\n\n* Network programming\n* High-concurrency Go architecture\n* Proxy technologies (HTTP, SOCKS4/5)\n* Traffic anonymisation & rotation patterns\n* Large-scale ingestion pipelines\n* Router behaviour & connection multiplexing\n* Multi-platform deployment\n* API design and client integration\n* Cross-language tooling (Go, Python, JS/Firefox extension)\n* Building secure, performance-critical infrastructure\n\nThe result is a **battle-hardened, high-throughput proxy router** capable of powering:\n\n* Web scraping farms\n* Security or penetration testing infrastructures\n* Research clusters\n* Privacy networks\n* Automated AI agents needing anonymised browsing\n* Local devices needing enhanced privacy\n\nAll running on **KNWS-owned infrastructure** with full control and no dependency on external proxy management services.",
    "technologies": {
      "frontend": [
        "html",
        "javascript"
      ],
      "backend": [
        "go"
      ],
      "database": [
        "sqlite",
        "mysql"
      ],
      "devops": [
        "docker"
      ],
      "tools": [
        "python",
        "firefox",
        "socks5"
      ]
    },
    "screenshot": "/screenshots/proxy-router.png"
  },
  {
    "name": "CoSearch",
    "url": "http://cosearch.knws.co.uk",
    "github": "https://github.com/datagram1/cosearch.git",
    "summary": "A high-performance, full-stack search platform indexing and querying over 4 million UK companies. Features ultra-fast MySQL FULLTEXT search with async Python, intelligent relevance ranking, detailed company profiles, and a clean front-end interface. Provides instant, paginated results with sub-100ms query performance across massive datasets.",
    "longDescription": "# üè¢ **CoSearch ‚Äì UK Company Search Engine**\n\n### *Long-Form Project Summary for KNWS Showcase*\n\n**CoSearch** is a high-performance, full-stack search platform designed to index, query, and visualise data for **over 4 million UK companies**, including their addresses, statuses, officers, registration details, SIC codes, and more.\nIt is built as a **real-time, big data lookup engine** that combines asynchronous Python, optimised MySQL FULLTEXT search, and a sleek front-end to provide instant, relevance-based results for users.\n\nThe system is similar in ambition to Companies House search ‚Äî but privately hosted, significantly faster, more flexible, and capable of running advanced ranking algorithms, bulk datasets, and enhanced metadata not offered by the official public APIs.\n\n---\n\n# üéØ **Project Goals**\n\n1. Build a **high-speed UK company search engine** over millions of records.\n2. Support rich search capabilities across name, address, number, postcode, officers, and other metadata.\n3. Deliver **instant, paginated, relevance-ranked results** even on large datasets.\n4. Provide a clean user interface for browsing company data.\n5. Maintain a highly optimised backend pipeline capable of handling large ingestion volumes.\n6. Ensure low-latency performance via async Python and database optimisations.\n7. Enable private, offline-capable searching without relying on external APIs.\n\n---\n\n# üß± **Core Features**\n\n## üîç **1. Massive Dataset: 4+ Million UK Companies**\n\nThe backend stores:\n\n* Company name\n* Company number\n* Address lines\n* Postcode and locality\n* SIC codes\n* Status (active, dissolved, liquidation)\n* Date of incorporation\n* Officer information\n* Filing history (optional extension)\n\nThe dataset is pre-indexed and kept in sync with Companies House bulk data.\n\n---\n\n## ‚ö° **2. Ultra-Fast Search (Async Python + FULLTEXT Indexing)**\n\nQueries run through:\n\n* MySQL FULLTEXT indexes\n* Advanced query tuning\n* Relevance scoring\n* Result pagination\n* Async connection pooling\n* Memory-optimised lookups\n\nThis allows sub-100ms queries even across millions of records.\n\nPerformance tech includes:\n\n* Async Python (`asyncio`, `aiomysql`)\n* Connection pools for high concurrency\n* Prepared statement caching\n* Ranking algorithms for fuzzy matches\n* Index alignment for instant lookup\n\n---\n\n## üß† **3. Intelligent Ranking & Relevance Engine**\n\nThe search engine uses:\n\n* Exact match prioritisation\n* Partial match fallback\n* Address and postcode weightings\n* Location-based scoring\n* Result de-duplication\n* Noise-word removal\n* Smart tie-breaking rules\n\nExamples:\n\n* \"Apple\" ‚Üí Apple UK companies first\n* \"12345\" ‚Üí direct match to a company number\n* \"London consulting ltd\" ‚Üí weighted towards location + keywords\n\n---\n\n## üè¢ **4. Detailed Company Profiles**\n\nEach company page shows:\n\n* Name\n* Status\n* Registered office address\n* Incorporation date\n* SIC codes\n* Officer data (directors / secretaries)\n* Filing information (if included)\n* Links to external sources\n* Related companies with shared officers\n\nView templates are optimised for readability and speed.\n\n---\n\n## üîÑ **5. High-Performance Import Pipeline**\n\nThe ingestion scripts allow:\n\n* Bulk import of millions of company records\n* Cleaning/normalisation of CSV datasets\n* Removal of duplicates\n* Automatic index optimisation\n* Scheduled updates\n\nThe system can rebuild the entire dataset quickly with minimal downtime.\n\n---\n\n## üñ•Ô∏è **6. Front-End UI**\n\nBuilt for clarity and speed:\n\n* Instant search feedback\n* Clean list-style results\n* Company cards with essential metadata\n* Mobile-friendly layout\n* Pagination for 100k+ results\n* Fast transitions between company pages\n\nThin, optimised HTML/CSS/JS ensures near-zero load time.\n\n---\n\n# üõ†Ô∏è **Technology Stack**\n\n### **Backend**\n\n* Python (async)\n* `asyncio`, `aiomysql`\n* MySQL or MariaDB for large dataset indexing\n* FULLTEXT search + custom ranking\n* Connection pooling\n* Bulk data importers (CSV ‚Üí MySQL pipeline)\n\n### **Frontend**\n\n* HTML5\n* Lightweight CSS\n* Async JS fetch calls\n* Optional Tailwind/vanilla CSS\n* Server-side rendering via Python templates\n\n### **Infrastructure**\n\n* Hosted on KNWS infrastructure\n* Systemd timers for scheduled sync\n* Log-based performance monitoring\n* Large-dataset-optimised MySQL configuration\n* Index inspection tools (`EXPLAIN`, `SHOW INDEX`)\n\n---\n\n# üìà **Outcome & Impact**\n\nCoSearch delivers:\n\n* **Lightning-fast search** over millions of entities\n* A reliable, private alternative to Companies House search\n* Advanced ranking logic not available publicly\n* Support for officer-based relationships and deep metadata\n* A platform suitable for:\n\n  * Business intelligence\n  * Fraud detection and due diligence\n  * Research and statistical analysis\n  * Sales prospecting\n  * Internal KNWS data exploration\n\nThis project demonstrates strong competency in:\n\n* Big-data engineering\n* Async Python and concurrency\n* Large-scale MySQL indexing and optimisation\n* Data ingestion pipelines\n* Search engine design (ranking, relevance, pagination)\n* Clean frontend UX for dense datasets",
    "technologies": {
      "frontend": [
        "html5",
        "css",
        "javascript"
      ],
      "backend": [
        "python",
        "asyncio",
        "aiomysql"
      ],
      "database": [
        "mysql"
      ],
      "devops": [],
      "tools": []
    },
    "screenshot": "/screenshots/cosearch.png"
  },
  {
    "name": "ATLAS",
    "url": null,
    "github": null,
    "summary": "A ground-up next-generation hybrid video codec combining classical signal processing with AI-assisted analysis, semantic modeling, and advanced multi-pass encoding. Features modular architecture with background reuse, semantic-aware compression, deterministic neural enhancement framework, and real-time GPU decoding. Includes full VLC/FFmpeg integration and automated VMAF testing dashboard.",
    "longDescription": "# üåê **ATLAS ‚Äî Next-Generation Hybrid Video Codec**\n\n### *Long-Form Project Summary for KNWS Showcase*\n\n**ATLAS** is a ground-up video codec designed to explore the boundaries of modern compression by combining classical signal-processing techniques with AI-assisted analysis, semantic modelling, and advanced multi-pass encoding. The project spans a full production toolchain: custom codec libraries, VLC and FFmpeg integration, GPU-optimised components, automated benchmarking, and a complete test infrastructure.\n\nATLAS was built to answer a single question:\n**How much can a codec improve when it understands the structure, meaning, and behaviour of the video itself?**\n\n---\n\n## üöÄ **Project Goals**\n\n1. **Design a new hybrid codec architecture** that supports classical DCT + motion vectors while layering modern techniques such as semantic analysis, background reuse, and deterministic neural enhancement.\n2. **Achieve competitive or better performance** compared to H.264/H.265 across a wide range of content.\n3. **Provide real-world tooling** so the codec can be used, tested, and validated inside standard video processing ecosystems.\n4. **Establish a development platform** capable of supporting experimental compression layers without breaking determinism.\n\n---\n\n# üß± **Core Architecture & Design**\n\nATLAS consists of multiple modular layers, each of which can be enabled, disabled, or combined depending on the encoding scenario. The architecture was intentionally designed so new layers can be added without destabilising the core codec engine.\n\n### **1. Classic Codec Backbone**\n\nA full traditional encoder/decoder pipeline, including:\n\n* I/P/B frames\n* Block-based transforms\n* DCT and quantisation\n* Motion search & compensation\n* Temporal prediction\n* rANS entropy coding\n\nThis provides a deterministic, battle-tested foundation that other layers build upon.\n\n---\n\n### **2. ATLAS Background Reuse Layer**\n\nA central innovation of the project.\n\nATLAS identifies static background regions, extracts them into an **atlas texture**, and reuses those blocks across frames with negligible additional cost.\nOnly changes, deltas, or new tiles need to be encoded.\n\nThis dramatically reduces bitrate on scenes with stationary backgrounds:\n\n* Interview content\n* News broadcasts\n* Films with long static shots\n* Gaming footage\n* Any content with static geometry\n\nThe atlas is referenced by tiles, tracked over time, and compressed independently.\n\n---\n\n### **3. Semantic-Aware Compression (SEM)**\n\nA lightweight, codec-embedded semantic analysis system designed to identify:\n\n* Static regions\n* Moving objects\n* Shot boundaries\n* Scene changes\n* Object reappearance\n* Meaningful motion vs unimportant noise\n\nThese signals inform the encoder so it can:\n\n* reduce bitrate in visually irrelevant areas\n* allocate more bits to complex or moving regions\n* trigger intra refreshes at shot boundaries\n* track important objects more efficiently\n\nSEM is a non-neural, deterministic feature extractor designed specifically for codec usage.\n\n---\n\n### **4. GEN ‚Äî Deterministic Neural Enhancement Layer (Framework Complete)**\n\nGEN establishes a framework for future neural enhancement models such as:\n\n* detail recovery\n* texture hallucination\n* denoising\n* super-resolution\n\nThe key innovation is that GEN is **deterministic**, meaning:\n\n* the same input always produces the same reconstructed frame\n* the codec remains bit-exact\n* distribution across platforms is safe\n\n(Models themselves are still pending; the framework, hooks, and build system are complete.)\n\n---\n\n### **5. TOK ‚Äî Token-Space Residuals (Planned Layer)**\n\nThis introduces **vector-quantised residuals** for:\n\n* ultra-low-bitrate scenarios\n* texture synthesis\n* perceptually-guided reconstruction\n\nTOK provides a foundation for extremely compact residual encoding, especially useful for AI-assisted generation of fine textures.\n\n---\n\n### **6. TND ‚Äî \"Tomorrow Never Dies\" Reverse-Pass Encoding**\n\nA novel multi-pass encoding strategy.\n\nTND uses a **reverse pass** to analyse frames *from the future* so the encoder can:\n\n* pre-allocate bits more intelligently\n* prioritise upcoming complex motion\n* insert keyframes with future context\n* stabilise VBR fluctuations\n\nThis mode is ideal for offline encoding workflows.\n\n---\n\n### **7. Licensing & Watermarking Layer**\n\nIncludes:\n\n* **ATLS atom** for embedding licensing metadata\n* **Deterministic watermarking** for forensic analysis\n* **Ed25519 signing pipeline**\n* **Token-based identification**\n\nThis allows each encoded file to carry tamper-proof metadata.\n\n---\n\n### **8. Real-Time GPU Decoding**\n\nDecoder-side optimisations for:\n\n* **Metal** (macOS)\n* **CUDA** (NVIDIA)\n* **Vulkan** (Linux/Windows cross-platform)\n\nThis ensures that ATLAS can run at high frame rates even at 4K+ resolutions.\n\n---\n\n# üõ† **Tooling & Integrations**\n\nATLAS includes full integration into industry-standard video tools.\n\n## **VLC v4 (Custom Build)**\n\nA custom distribution of VLC 4 with:\n\n* native ATLAS decoder module\n* tile atlas visualisation tools\n* debug overlays\n* extended media info for ATLS atoms\n\nThis allows real-world playback of ATLAS files.\n\n---\n\n## **FFmpeg (Custom Build)**\n\nFFmpeg was patched to add:\n\n* `libatlas` encoder\n* `libatlas` decoder\n* atlas-specific `AVCodecContext` fields\n* custom `AVFrame` metadata\n* seamless integration with filters and tools\n\nThis enables:\n\n* command-line encoding\n* piping from other tools\n* automated CI testing\n* benchmark comparisons with H.264/H.265\n\n---\n\n# üß™ **Automated Testing & Flask Web System**\n\nATLAS includes a full **Flask-based testing suite**:\n\n* Upload or reference test videos\n* Automatically encode using ATLAS and H.264/H.265 baselines\n* Automatically run Netflix **VMAF**\n* Produce charts, deltas, data tables, regression comparisons\n* Track improvements or regressions across versions\n* Run custom scenarios such as:\n\n  * static backgrounds\n  * high-motion content\n  * animation\n  * colour-sensitive scenes\n  * artificial stress tests\n\nThe dashboard is used daily for development and performance tuning.\n\n---\n\n# üß∞ **Build System & Cross-Platform Support**\n\nATLAS supports:\n\n* macOS (Intel + ARM)\n* Linux (x86 + ARM)\n* Windows (MSVC + MinGW)\n* GPU-enabled builds\n* headless encoder mode\n* CI-driven binary releases\n\nThe build uses a customised CMake + scripting pipeline that orchestrates:\n\n* Codec library builds\n* Plugin assembly for VLC and FFmpeg\n* GPU kernel builds\n* Static and shared-library variants\n* Release packaging\n* Regression test snapshots\n\n---\n\n# üìà **Outcome & Impact**\n\nThe project demonstrates:\n\n* the ability to design a full-scale video codec\n* understanding of compression theory, GPU systems, and multimedia toolchains\n* end-to-end engineering from core algorithm design to distribution tooling\n* practical interoperability with industry tools\n* real performance testing against standard frameworks (Netflix VMAF)\n* a modular foundation capable of future research and extensions\n\nATLAS is already able to encode and decode real-world content, integrate with FFmpeg and VLC, run across multiple platforms, and serve as a research platform for advanced hybrid compression techniques.",
    "technologies": {
      "backend": [
        "c++",
        "python",
        "cmake"
      ],
      "database": [
        "sqlite3",
        "sqlalchemy"
      ],
      "devops": [
        "docker",
        "ci/cd"
      ],
      "tools": [
        "ffmpeg",
        "vlc",
        "vmaf",
        "metal",
        "cuda",
        "vulkan"
      ]
    },
    "screenshot": "/screenshots/atlas.jpg"
  },
  {
    "name": "KNWS Website",
    "url": null,
    "github": null,
    "summary": "Modern full-stack website showcasing KNWS's portfolio and services. Built with React/TypeScript frontend and Flask backend, featuring a responsive design, project showcase with filtering, and markdown support.",
    "longDescription": "# KNWS Website - Project Architecture\n\nA modern, full-stack website showcasing KNWS's portfolio and services, built with React/TypeScript frontend and Flask backend.\n\n## üèóÔ∏è Architecture Overview\n\nThis project combines a **React SPA (Single Page Application)** frontend with a **Flask** backend, designed to showcase KNWS's custom software development services and project portfolio.\n\n### Frontend (React/TypeScript)\n- **Framework**: React 18 with TypeScript\n- **Routing**: React Router v6 for client-side navigation\n- **Styling**: Tailwind CSS with shadcn/ui components\n- **Build Tool**: Create React App (CRA)\n- **State Management**: React hooks (useState, useEffect)\n\n### Backend (Flask/Python)\n- **Framework**: Flask 2.0\n- **Templates**: Jinja2 for server-side rendering\n- **Static Assets**: Serves legacy HTML templates and static files\n\n## üìÅ Project Structure\n\nThe project is organized into clear frontend and backend sections:\n\n- **`src/`**: React frontend source code with components, data, and utilities\n- **`web/`**: Flask backend with templates and static assets\n- **`public/`**: Public assets served by React, including project screenshots\n- **`scripts/`**: Automation scripts for fetching project info and taking screenshots\n\n## üõ†Ô∏è Technology Stack\n\n### Frontend Technologies\n- **React 18.2.0**: Modern React with hooks and concurrent features\n- **TypeScript 4.9.5**: Type-safe JavaScript\n- **React Router DOM 6.20.0**: Client-side routing\n- **Tailwind CSS 3.3.6**: Utility-first CSS framework\n- **shadcn/ui**: High-quality React component library built on Radix UI primitives\n- **Lucide React**: Modern icon library\n- **React Markdown 10.1.0**: Markdown rendering for project descriptions\n- **class-variance-authority**: Component variant management\n- **clsx & tailwind-merge**: Conditional class utilities\n\n### Backend Technologies\n- **Flask 2.0.1**: Lightweight Python web framework\n- **Jinja2 3.0.1**: Template engine for server-side rendering\n- **Werkzeug 2.0.1**: WSGI utilities\n- **Python 3.x**: Backend runtime\n\n### Development Tools\n- **Create React App**: React build tooling and dev server\n- **Playwright 1.40.0**: Browser automation for screenshots\n- **PostCSS & Autoprefixer**: CSS processing\n- **ESLint**: Code linting (React app config)\n- **TypeScript Compiler**: Type checking and compilation\n\n## üé® Design System\n\n### Component Architecture\nThe project uses **shadcn/ui** components built on **Radix UI** primitives:\n- **Accessible**: Built with ARIA attributes and keyboard navigation\n- **Customizable**: Styled with Tailwind CSS, easy to theme\n- **Composable**: Small, reusable components\n\n### Styling Approach\n- **Tailwind CSS**: Utility-first CSS for rapid development\n- **CSS Custom Properties**: Theme variables for consistent theming\n- **Responsive Design**: Mobile-first approach with breakpoints\n- **Dark Mode Ready**: Color scheme supports dark backgrounds\n\n## üîÑ Key Features\n\n### 1. Project Showcase\n- Dynamic filtering by technology\n- Modal-based project details\n- Markdown support for rich descriptions\n- Screenshot gallery\n- Learn More functionality for detailed project information\n\n### 2. Responsive Design\n- Mobile-first approach\n- Breakpoints: sm, md, lg, xl\n- Touch-friendly interactions\n\n### 3. Performance Optimizations\n- Code splitting via React Router\n- Lazy loading for images\n- Optimized bundle size\n\n### 4. Accessibility\n- Semantic HTML\n- ARIA labels and roles\n- Keyboard navigation\n- Screen reader friendly\n\n## üöÄ Development Workflow\n\n### Frontend Development\n```bash\nnpm start              # Start React dev server (localhost:3000)\nnpm run start:debug    # Debug mode with source maps\nnpm run build          # Production build\n```\n\n### Backend Development\n```bash\npython web/app.py      # Start Flask dev server (localhost:5030)\n```\n\n### Automation Scripts\n```bash\nnpm run fetch-projects    # Fetch GitHub project info\nnpm run screenshot        # Take screenshots of projects\n```\n\n## üì¶ Data Management\n\n### Projects\n- Stored in `src/data/projects.json`\n- Can be updated manually or via `fetch-projects` script\n- Supports project metadata, technology tags, screenshots, and markdown descriptions\n\n### Content\n- Services and industries defined in React components\n- Easy to update in component files\n- No CMS required for static content\n\n## üåê Deployment\n\n### Production Build\n1. **Frontend**: `npm run build` creates optimized `build/` folder\n2. **Backend**: Flask app serves static files and API routes\n3. **Apache**: Configured via `apache_flask.conf` for WSGI\n\n## üìù Architecture Highlights\n\n- **Hybrid Architecture**: Combines React SPA with Flask backend for flexibility\n- **Component-Based**: Modular React components for maintainability\n- **Type-Safe**: TypeScript throughout for better developer experience\n- **Modern Stack**: Latest versions of React, TypeScript, and Tailwind CSS\n- **Developer Experience**: Hot reload, TypeScript, ESLint, and automation scripts",
    "technologies": {
      "frontend": [
        "react",
        "typescript",
        "javascript"
      ],
      "backend": [
        "python",
        "flask",
        "jinja2"
      ],
      "devops": [
        "apache",
        "wsgi"
      ],
      "tools": [
        "tailwindcss",
        "playwright",
        "npm",
        "postcss",
        "eslint"
      ]
    },
    "screenshot": "/screenshots/knws-showcase.png"
  }
]